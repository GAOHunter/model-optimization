{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quantization Aware Training in Keras Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjmi3qZeu_xk",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Welcome to an end-to-end MNIST example for *quantization-aware training*.\n",
        "\n",
        "### Other Pages\n",
        "For an introduction to what quantization-aware-training is and to determine if you want to use it, see the [overview page](https://www.tensorflow.org/model_optimization/guide/quantization/training.md).\n",
        "\n",
        "To quickly finding the APIs you need for your use case (beyond the single path in this example), see the\n",
        "[comprehensive guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide.md).\n",
        "\n",
        "### Contents\n",
        "\n",
        "In this tutorial, we will:\n",
        "\n",
        "1.   Train a tf.keras MNIST from scratch. TODO: use pre-trained instead. We cannot use a GPU given TFLite part also.\n",
        "\n",
        "2.   Apply quantization-aware training API to MNIST, see the accuracy, and\n",
        "     export a quantization-aware model.\n",
        "\n",
        "3.   Use the model to create an actually quantized model for the TFLite\n",
        "     backend. \n",
        "\n",
        "4.   See the 4x model size reduction and the persistence of accuracy in \n",
        "     TFLite. To see the latency benefits on mobile, try out the TFLite examples [in the TFLite app repository](https://www.tensorflow.org/lite/models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN4yVFK5-0Bf",
        "colab_type": "code",
        "outputId": "1b1c81d2-592f-4cd3-de6c-ea278f34b216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip install -q tf-nightly==2.2.0.dev20200305\n",
        "! pip install -q --extra-index-url=https://test.pypi.org/simple/ tensorflow-model-optimization==0.3.0.dev3\n",
        "\n",
        "import tempfile\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[K     |████████████████████████████████| 516.8MB 32kB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 60.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 49.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 54.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 778kB 31.0MB/s \n",
            "\u001b[31mERROR: tensor2tensor 1.14.1 requires bz2file, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensor2tensor 1.14.1 requires gevent, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensor2tensor 1.14.1 requires gunicorn, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensor2tensor 1.14.1 requires kfac, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensor2tensor 1.14.1 requires mesh-tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensor2tensor 1.14.1 requires pypng, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensor2tensor 1.14.1 requires tensorflow-datasets, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensor2tensor 1.14.1 requires tensorflow-gan, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.1.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.1.0 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.1.0 has requirement tensorflow-estimator<2.2.0,>=2.1.0rc0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 163kB 5.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 8.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psViY5PRDurp",
        "colab_type": "text"
      },
      "source": [
        "# Train a MNIST model without quantization-aware-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbY-KGMPvbW9",
        "colab_type": "code",
        "outputId": "cf09451c-ff4c-428f-991b-357a00c90895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define the model architecture. \n",
        "# TODO(tfmot): change back to Sequential once CL is submitted.\n",
        "inp = keras.layers.Input(shape=(28, 28))\n",
        "x = keras.layers.Reshape(target_shape=(28, 28, 1))(inp)\n",
        "x = keras.layers.Conv2D(32, 5, padding='same', activation='relu')(x)\n",
        "x = keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same')(x)\n",
        "x = keras.layers.Conv2D(64, 5, padding='same', activation='relu')(x)\n",
        "x = keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same')(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "x = keras.layers.Dense(1024, activation='relu')(x)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "out = keras.layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = keras.models.Model([inp], [out])\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=1,\n",
        "  validation_data=(test_images, test_labels)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1086 - accuracy: 0.9656 - val_loss: 0.0360 - val_accuracy: 0.9879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f40803404a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8747K9OE72P",
        "colab_type": "text"
      },
      "source": [
        "# Apply quantization-aware-training to the pre-trained MNIST.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F19k7ExXF_h2",
        "colab_type": "text"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsZROpNYMWQ0",
        "colab_type": "text"
      },
      "source": [
        "We apply quantization-aware training to the whole model and see this in the model summaries. All layers are now prefixed by \"quant\". In the [comprehensive guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide.md), you can see how to quantize some layers for model accuracy improvements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq6blGjgFDCW",
        "colab_type": "code",
        "outputId": "e2cf4084-f3e5-4508-c1b8-294ac79d093e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
            "_________________________________________________________________\n",
            "quantize_layer (QuantizeLaye (None, 28, 28)            3         \n",
            "_________________________________________________________________\n",
            "quant_reshape (QuantizeWrapp (None, 28, 28, 1)         1         \n",
            "_________________________________________________________________\n",
            "quant_conv2d (QuantizeWrappe (None, 28, 28, 32)        899       \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 32)        1         \n",
            "_________________________________________________________________\n",
            "quant_conv2d_1 (QuantizeWrap (None, 14, 14, 64)        51395     \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 64)          1         \n",
            "_________________________________________________________________\n",
            "quant_flatten (QuantizeWrapp (None, 3136)              1         \n",
            "_________________________________________________________________\n",
            "quant_dense (QuantizeWrapper (None, 1024)              3212293   \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 1024)              1         \n",
            "_________________________________________________________________\n",
            "quant_dense_1 (QuantizeWrapp (None, 10)                10257     \n",
            "=================================================================\n",
            "Total params: 3,274,852\n",
            "Trainable params: 3,274,634\n",
            "Non-trainable params: 218\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDr2ijwpGCI-",
        "colab_type": "text"
      },
      "source": [
        "## Train and evaluate the model against baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PHDGJryE31X",
        "colab_type": "code",
        "outputId": "7937d1a7-6c6f-4f95-a51e-197b739dad9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "q_aware_model.fit(train_images, train_labels, batch_size=500, epochs=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 3s 27ms/step - loss: 0.0250 - accuracy: 0.9925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f40664b9710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-byC2lYlMkfN",
        "colab_type": "text"
      },
      "source": [
        "For this example, the test accuracies are similar before and after."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bMFTKSSHyyZ",
        "colab_type": "code",
        "outputId": "46924767-0b79-4c80-b913-5a0d7d5c4f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    test_images, test_labels, verbose=0)\n",
        "\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "   test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline test accuracy: 0.9879000186920166\n",
            "Quant test accuracy: 0.9915000200271606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IepmUPSITn6",
        "colab_type": "text"
      },
      "source": [
        "# Create quantized model for TFLite backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FgNP4rbOLH8",
        "colab_type": "text"
      },
      "source": [
        "After this, we have an actually quantized model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7fztWsAOHTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEYsyYVqNgeY",
        "colab_type": "text"
      },
      "source": [
        "# See 4x model size reduction and persistence of accuracy in TFLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saadXD4JQsBK",
        "colab_type": "text"
      },
      "source": [
        "We define a helper function to evaluate the TF Lite model the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8yBouuGNqls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for test_image in test_images:\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  accurate_count = 0\n",
        "  for index in range(len(prediction_digits)):\n",
        "    if prediction_digits[index] == test_labels[index]:\n",
        "      accurate_count += 1\n",
        "  accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuEFS4CIQvUw",
        "colab_type": "text"
      },
      "source": [
        "We evaluate the quantized model and see that the accuracy from TensorFlow persists to the TFLite backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqQTyqz4NsWd",
        "colab_type": "code",
        "outputId": "cdc0cc7c-27f2-422d-eb6b-1a521ba9e795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quant TFLite test_accuracy: 0.9912\n",
            "Quant TF test accuracy: 0.9915000200271606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1c2IecBRCdQ",
        "colab_type": "text"
      },
      "source": [
        "We create a float TFLite model and then see that the quantized TFLite model\n",
        "is 4x smaller."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy_Lgfh8VkyX",
        "colab_type": "code",
        "outputId": "2fbecdfd-a642-4333-ebff-031b43e4d49b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Create float TFLite model.\n",
        "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "float_tflite_model = float_converter.convert()\n",
        "\n",
        "_, float_file = tempfile.mkstemp('.tflite')\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "with open(float_file, 'wb') as f:\n",
        "  f.write(float_tflite_model)\n",
        "\n",
        "import os \n",
        "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Float model in Mb: 12.494964599609375\n",
            "Quantized model in Mb: 3.1320648193359375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O5xuci-SonI",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2I7xmyMW5QY",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial, we showed you how to create *quantization-aware models* with the TensorFlow Model Optimization Toolkit API and then *quantized models* in the TFLite backend. \n",
        "\n",
        "We saw a 4x model size compression benefit for MNIST, with minimal accuracy\n",
        "difference. To see the latency benefits on mobile, try out the TFLite examples [in the TFLite app repository](https://www.tensorflow.org/lite/models).\n",
        "\n",
        "We encourage you to try this new capability, which can be particularly important for deployment in resource-constraint environments. \n",
        "\n"
      ]
    }
  ]
}